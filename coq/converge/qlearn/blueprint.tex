\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{float}
% \usepackage{ebgaramond-maths}
\usepackage{fullpage}	
\usepackage{amsfonts}
\usepackage{color}
\usepackage{lipsum}
\usepackage{wrapfig}
\usepackage{amsmath, amssymb, amsfonts, amscd, amsthm, mathrsfs}
\usepackage{tabularx}
\usepackage{framed}
\usepackage{longtable,tabu}
\usepackage{hyperref}
\usepackage{pifont}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% \usepackage{pgfplots}
% \pgfplotsset{compat=1.15}
% \usepackage{mathrsfs}
% \usetikzlibrary{arrows}
% \pagestyle{empty}


% \theoremstyle{plain}
% \usepackage{ntheorem}

\title{Q-learning convergence : A Formal Blueprint}
\author{Koundinya Vajjha}

\def\R{\mathbb{R}}
\def\dmin{D_{\min}}
\def\Cinf{C^\infty(M)}
\def\star{\mathfrak{h}^{\ast}}
\def\E{\mathbb{E}}
\def\act{\mathcal{A}}
\def\rew{\mathcal{R}}

\newcommand{\figurehere}[2]{\smallskip\begin{center} 
\includegraphics[width=#1\textwidth]{#2}\end{center}\smallskip}

\newcommand{\image}[1]{\smallskip\begin{center} #1 \end{center}\smallskip}
\newcommand{\red}[1]{\textcolor{red}{#1}}
% environments for theorems, lemmas, etc.
\newtheorem{lem}{Lemma}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{thm}{Theorem}
\newtheorem{ex}[lem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{conjecture}{Conjecture}
\newtheorem*{remark}{Remark:}
% \newtheorem{lem}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{defn}[theorem]{Definition}
\newcommand{\code}[1]{\texttt{#1}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\fleur}{\ding{95}}
\newcommand{\HTMLBase}{https://certrl.github.io/CertRLanon}
\newcommand{\coqHTMLBase}{\HTMLBase}
\newcommand{\coqBaseModule}{CertRL.}
\newcommand{\coqtop}{\text{\href{https://github.com/CertRL/CertRLanon}{\fleur}}}
\newcommand{\coqdef}[2]{\text{\href{\coqHTMLBase/\coqBaseModule#1.html\##2}{\fleur}}}



\begin{document}

\maketitle

\section{Introduction}
This will be a formal blueprint of the convergence proof for Q-learning, following the original 1992 proof of Watkins-Dayan. Formal equivalents, whenever available, shall be linked to by a \coqtop.

In a nutshell, the proof proceeds by constructing an auxiliary Markov Decision Process, called the \textit{action-replay} process (ARP) associated to any given MDP. The ARP is constructed so that it's optimal $Q$-values (defined below) approximate the optimal $Q$-value of the parent MDP. 
A stochastic approximation result (the Robbins-Monro procedure) finally shows that the approximations indeed converge to the optimal value.

We start out with an MDP $M$ \coqdef{converge.mdp}{MDP} which has 
\begin{itemize}
    \item A nonempty, finite state space $X$.
    \item An nonempty, finite action space $\act$ (fibered over each state $x:X$). 
    \item A stochastic transition structure $T$ which assigns a probability measure over the state space given a state $x$ and an action $a : \act(x)$ available at that state. 
    \item A reward function $r$. \coqdef{converge.mdp}{reward}
\end{itemize}
The expected reward after one transition from state $x$ after taking an action $a$ is called the \textit{step expected reward} \coqdef{converge.mdp}{step_expected_reward} and is denoted $\rew_x(a)$

A \textit{decision rule}\coqdef{converge.mdp}{dec_rule} is a map from the state space to the action space. Transitions happen after actions are taking in a particular state space following a decision rule.
Denote by $T_\pi (x) := T(x,\pi(x))$ which is the probability measure on $X$ which results after one transition following the decision rule $\pi$ at state $x$. 

If we use the same decision rule at each time step $n$, such policies are called \textit{stationary}. Consider only stationary policies from now. 

The long-term value \coqdef{converge.mdp}{ltv} of a policy $\pi$ is denoted by $V^{\pi}$ and is shown to be equal to
\[ 
V^{\pi}(x) = \rew_x(\pi(x)) + \gamma \E_{T_{\pi}(x)} [V^{\pi}]
\]
where $0 \le \gamma \le 1$ is a discount factor and the expectation is with respect to the probability measure $T_{\pi}(x)$.

\begin{defn}[$Q$-value]
Given an MDP $M$ and a policy $\pi$, we define the $Q$-value as:
\[ 
Q^{\pi}(x,a) := \rew_x(a) + \gamma \E_{T(x,a)}V^{\pi} 
\]
\end{defn}

The value and policy iteration algorithms give us an \textit{optimal policy} denoted $\pi^*$ which maximizes the long-term value. This value is called the optimal value function denoted $V^{\pi^*} := \max_{\pi}(V^\pi)$. \coqdef{converge.mdp}{max_ltv}

Define $Q^*(x,a) := Q^{\pi^*}(x,a)$. We can show that 
\[ 
V^*(x) := V^{\pi^*}(x) = \max_{a}\left( \rew_x(a) + \gamma\E_{T(x,a)}V^{\pi^*}\right) = \max_{a}Q^{\pi^*}(x,a) = \max_{a}Q^*(x,a)
\]
which says that the optimal value function is the fixed point of the optimal Bellman operator. \coqdef{converge.mdp}{max_ltv_eq_fixpt}

So if an agent can learn the $Q^*$ value, then we can learn $\pi^*$ by simply taking $\pi^*(x) = \argmax_{a} Q^*(x,a)$.

The evolution of an MDP proceeds in stages. At the $n$-th stage, the agent observes state $x_n$, takes an action $a_n$, observes a subsequent stage $y_n$, receives an immediate payoff $r_n$ and adjusts it's $Q_{n-1}$ value (defined below) using a learning factor $\alpha_n$. 

\begin{defn}[$Q_n$-value]
Assume that $Q_0(x,a)$ is a known function for all $x : X$ and $a : \act(x)$. 
For $0 \le \alpha_n < 1$ define inductively the following sequence 
\[Q_n(x,a) :=
\begin{cases}
(1 - \alpha_n)Q_{n-1}(x,a) + \alpha_n\left[ r_n + \gamma V_{n-1}(y_n) \right] \mathrm{ if } \ x=x_n \ \mathrm{ and } \ a=a_n \\
Q_{n-1}(x,a) \quad \mathrm{ otherwise}
\end{cases}
\]
where $r_n = r(x_n,a_n)$ and $V_{n-1}(y) = \max_a Q_{n-1}(y,a)$.
\end{defn}
In practice, the $Q_0$ value is set to zero identically. 
$Q$-learning states that under certain regularity conditions on the learning rate, the sequence of $Q_n$ values converges to $Q^*$ with probability 1. 

\begin{theorem} [$Q$-learning convergence]
Fix a state $x:X$ and action $a : \act(x)$. 
If we have, for all $x,a$ that
\[
\sum_{i}\alpha_{n^i(x,a)} = \infty \ , \ \sum_{i}\alpha_{n^i(x,a)}^2 < \infty \ 
\]
then we have $Q_n(x,a) \rightarrow Q^*(x,a)$ with probability 1. 
\end{theorem}

All random variables in sight and all probability measures are discrete. 

\section{Action-Replay Process Lemmas}
To prove this theorem, we set up an auxilary MDP called the Action-Replay Process (ARP). 
\subsection{Definition of the ARP.}
\begin{defn}[$i$-th occurence counter]
Given an MDP, let $n^i(x,a)$ denote the time when action $a$ is tried at state $x$ for the $i$-th time. 
\end{defn}

\begin{defn}[Action-Replay Process]
Given an MDP $M = (X,A,T,r)$, we define the action-replay process ARP as having 
\begin{itemize}
    \item State space $(\mathbb{N}\times X) \cup \{\ast\}$. Where $\ast$ is a special absorbing state.
    \item Action space $A$ is the same as the action space of $M$.
    \item Transition structure TODO. 
\end{itemize}
\end{defn}

\subsection{Optimal $Q$ values of the ARP.}

\section{Stochastic Convergence Lemmas}
\subsection{Convergence in $L^2 \Rightarrow$ Convergence with probability 1.}
\textbf{Note}: \red{This section may be optional. Maybe we should think about stating our convergence results just in $L^2$, since convergence in probability doesn't really add much?}

\begin{lemma}[Markov Inequality]
Let $X$ be a discrete random variable which is nonnegative. Then for any positve real number $a$, we have 
\[ 
P(X \ge a) \le \frac{\E X}{a}
\]
\end{lemma}
\begin{proof}
We have $\E X= \sum_{x} x P(X = x)$ \coqdef{converge.cond_expt}{expt_value_eq_class} and we now write:
\begin{align}
    \E X &= \sum_{x \ge a} x P(X = x) + \sum_{x < a} x P(X = x) \\
        &\ge \sum_{x \ge a} x P(X=x) + 0 \\
        &\ge a \sum_{x \ge a} P(X = x) \\
        &= a P(X \ge a)
\end{align}
where we used the nonnegativity of $X$ in the second step. 
\end{proof}

Using the Markov inequality, we can prove that convergence in $L^2$ implies convergence in probability. 

\begin{defn}[Convergence in $L^2$]
A sequence of random variables $X_n$ is said to converge in $L^2$ to a random variable $X$ if \[ 
\lim_{n \to \infty} \E |X_n - X|^2 = 0.
\]
\end{defn}

\begin{defn}[Convergence with probability 1]
A sequence of random variables $X_n$ is said to converge with probability 1 to a random variable $X$ if
\[
    \lim_{n \to \infty} P \left( | X_n - X | \ge \epsilon \right) = 0.
\]
\end{defn}
\begin{theorem}
Convergence in $L^2$ implies convergence in probability. 
\end{theorem}
\begin{proof}
For any $\epsilon > 0$, we have 
\begin{align}
    P(|X_n - X|) \ge \epsilon) &= P(|X_n - X|^2 \ge \epsilon^2) \\
    &\le \frac{\E |X_n - X|^2}{\epsilon}
\end{align}
The RHS now goes to 0 since $X_n \to X$ in $L^2$. 
\end{proof}

\subsection{Dvoretzky's convergence lemma}
This lemma is the essential lemma needed for convergence of $Q$-learning. 
\begin{lemma}\label{lem:qlearn-ess}
Let $\{X_n\}$ be discrete random variables updated as
\begin{equation}\label{eq:rob-monro}
X_{n+1} = X_n + \alpha_n (Y_n - X_n)    
\end{equation}
where $ 0 \le \alpha_n < 1$ are real numbers such that $\sum_n \alpha_n = \infty$ and $\sum_n \alpha_n^2 < \infty$ and $Y_n$ are bounded random variables with mean $\theta$. Then we have that $X_n \to \theta$ in $L^2$.
\end{lemma}

\begin{remark}
We note that
\begin{itemize}
    \item Convergence is also in probability if we use the lemmas in the previous section.
    \item The above recurrence scheme (\ref{eq:rob-monro}) is the Robbins-Monro scheme with $f(x) = x$. (See Dvoretzky's paper on page 50).
\end{itemize} 
\end{remark}

To prove this lemma, we use the work we did to prove the Dvoretzky convergence lemma, which we recall:
Let $T$ be a function satisfying the following assumption:
\[ 
|T_n(r_1,\dots,r_n) - \theta| \le F_n | r_n - \theta |
\]
Define $\{X_n\}$ as
\[
X_{n+1}(\omega) = T_n(X_1(\omega),\dots,X_n(\omega)) + Z_n(\omega) \qquad n \ge 1 
\]
assume also that
\begin{itemize}
    \item $\mathbb{E}[X_1^2] < \infty$,
    \item $\sum_{n=1}^{\infty}\E[Z_n^2] < \infty$,
    \item $\E[Z_n | x_1,\dots,x_n] = 0$ for all $n$ with probability 1,
\end{itemize}

Now if we set $V_n^2 = \E\left[ (X_n - \theta)^2\right]$ and $\sigma_n^2 = \E\left[ Z_n^2 \right]$, then (denoting $T_n$ for $T_n(X_1,\dots,X_n)$) we have:
\[ V_{n+1}^2 \le F_n^2 V_n^2 + \sigma_n^2\]

\begin{theorem}
Let $\{F_n\}$ be a sequence of positive real numbers such that $\prod_{n=0}^\infty F_n =0$. Assume we have a sequence of real numbers $\{V_n\}$ which satisfy for every $n$:
\[ 
    V_{n+1}^2 \le F_n^2 V_n^2 + \sigma_n^2
\]
Where $\{\sigma_n\}$ satisfy $\sum_{n=0}^\infty \sigma_n^2 < \infty$. Then we have that $V_n^2 \rightarrow 0$.
\end{theorem}

First off, note that in Lemma \ref{lem:qlearn-ess}, we can assume, without loss of generality, that $\theta = 0$. 
This is because we can rewrite (\ref{eq:rob-monro}) as 
\[ 
    (X_{n+1} - \theta)= (X_n - \theta) + \alpha_n ((Y_n-\theta) - (X_n-\theta))    
\] 
So, to prove Lemma \ref{lem:qlearn-ess}, we apply the Dvoretzky convergence lemma with $Z_n = \alpha_n Y_n$ and $T_n = (1 - \alpha_n)X_n$, which implies that $F_n = (1 - \alpha_n)$. 

Note that $$\sum_n\E[Z_n^2] = \sum_n \alpha_n^2 \E[Y_n^2] \le D \sum_n \alpha_n^2 < \infty$$ since we assume that $Y_n$ are bounded. 

The missing ingredient is supplied by this lemma:
\begin{lemma}
If we have $\sum_{n=0}^\infty \alpha_n = \infty$ where $0 \le \alpha_n < 1$ and if $F_n = 1 - \alpha_n$, then $\prod_{n=0}^\infty F_n = 0$. 
\end{lemma}
\begin{proof}
    We have $F_n = 1 - \alpha_n \le e^{-\alpha_n}$ (this result is proven in \texttt {RealAdd.v}) and so,
    \[ 
    0 \le \prod_{n=0}^k (1 - \alpha_k) \le e^{- \sum_{n=0}^k \alpha_k}
    \]
    By the hypothesis, the RHS goes to zero as $k \to \infty$. So, by the sandwich theorem we have $\prod_{n=0}^\infty F_n = 0$. 
\end{proof}

\end{document}

